{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Activation, Dropout, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-04 23:55:00,354] Making new env: CarRacing-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CarRacing-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 913..1153 -> 240-tiles track\n",
      "frame:0 reward:8.268200836820084 done:False\n",
      "frame:1 reward:-0.09999999999999964 done:False\n",
      "frame:2 reward:-0.09999999999999964 done:False\n",
      "frame:3 reward:-0.09999999999999964 done:False\n",
      "frame:4 reward:-0.09999999999999964 done:False\n",
      "frame:5 reward:-0.09999999999999964 done:False\n",
      "frame:6 reward:-0.09999999999999964 done:False\n",
      "frame:7 reward:-0.09999999999999964 done:False\n",
      "frame:8 reward:-0.09999999999999964 done:False\n",
      "frame:9 reward:-0.09999999999999964 done:False\n",
      "frame:10 reward:-0.09999999999999964 done:False\n",
      "frame:11 reward:-0.09999999999999964 done:False\n",
      "frame:12 reward:-0.09999999999999964 done:False\n",
      "frame:13 reward:-0.09999999999999964 done:False\n",
      "frame:14 reward:-0.09999999999999964 done:False\n",
      "frame:15 reward:-0.09999999999999964 done:False\n",
      "frame:16 reward:-0.09999999999999964 done:False\n",
      "frame:17 reward:-0.09999999999999964 done:False\n",
      "frame:18 reward:-0.09999999999999964 done:False\n",
      "frame:19 reward:-0.09999999999999964 done:False\n",
      "frame:20 reward:-0.09999999999999964 done:False\n",
      "frame:21 reward:-0.09999999999999964 done:False\n",
      "frame:22 reward:-0.09999999999999964 done:False\n",
      "frame:23 reward:-0.09999999999999964 done:False\n",
      "frame:24 reward:-0.09999999999999964 done:False\n",
      "frame:25 reward:-0.09999999999999964 done:False\n",
      "frame:26 reward:-0.09999999999999964 done:False\n",
      "frame:27 reward:4.084100418410042 done:False\n",
      "frame:28 reward:-0.09999999999999964 done:False\n",
      "frame:29 reward:-0.09999999999999964 done:False\n",
      "frame:30 reward:-0.09999999999999964 done:False\n",
      "frame:31 reward:-0.09999999999999964 done:False\n",
      "frame:32 reward:-0.09999999999999964 done:False\n",
      "frame:33 reward:-0.09999999999999964 done:False\n",
      "frame:34 reward:-0.09999999999999964 done:False\n",
      "frame:35 reward:-0.09999999999999964 done:False\n",
      "frame:36 reward:-0.09999999999999964 done:False\n",
      "frame:37 reward:-0.09999999999999964 done:False\n",
      "frame:38 reward:-0.09999999999999964 done:False\n",
      "frame:39 reward:-0.09999999999999964 done:False\n",
      "frame:40 reward:-0.09999999999999964 done:False\n",
      "frame:41 reward:-0.09999999999999964 done:False\n",
      "frame:42 reward:-0.09999999999999964 done:False\n",
      "frame:43 reward:-0.09999999999999964 done:False\n",
      "frame:44 reward:-0.09999999999999964 done:False\n",
      "frame:45 reward:-0.09999999999999964 done:False\n",
      "frame:46 reward:-0.09999999999999964 done:False\n",
      "frame:47 reward:-0.09999999999999964 done:False\n",
      "frame:48 reward:-0.09999999999999964 done:False\n",
      "frame:49 reward:-0.09999999999999964 done:False\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i in range(50):\n",
    "    # env.render()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, prob = env.step(action)\n",
    "    print('frame:{} reward:{} done:{}'.format(i, reward, done))\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 96, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.shape)\n",
    "print(env.action_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.09762701  0.71518937  0.60276338]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Params\n",
    "dropout = 0.8\n",
    "learning_rate = 0.001\n",
    "memory_size = 1000\n",
    "\n",
    "episodes = 10\n",
    "time = 3000\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.999\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 31, 31, 32)\n",
      "(?, 9, 9, 64)\n"
     ]
    }
   ],
   "source": [
    "bot_racer = Sequential()\n",
    "\n",
    "bot_racer.add(Conv2D(32, (3, 3), activation='relu', input_shape=(96, 96, 3)))\n",
    "bot_racer.add(MaxPooling2D(pool_size=(3,3)))\n",
    "bot_racer.add(Dropout(dropout))\n",
    "\n",
    "print(bot_racer.output.shape)\n",
    "\n",
    "bot_racer.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "bot_racer.add(MaxPooling2D(pool_size=(3,3)))\n",
    "bot_racer.add(Dropout(dropout))\n",
    "\n",
    "print(bot_racer.output.shape)\n",
    "\n",
    "bot_racer.add(Flatten())\n",
    "bot_racer.add(Dense(64, activation='relu'))\n",
    "bot_racer.add(Dropout(dropout))\n",
    "bot_racer.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "bot_racer.compile(loss='mse', optimizer=Adam(learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "memory = deque(maxlen=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1262..1578 -> 316-tiles track\n",
      "episode:1 frames:1000 total rewards:-39.68253968254033 epsilon:1\n",
      "Track generation: 1171..1468 -> 297-tiles track\n",
      "episode:2 frames:1000 total rewards:-29.054054054054426 epsilon:0.999\n",
      "Track generation: 1228..1539 -> 311-tiles track\n",
      "episode:3 frames:1000 total rewards:-35.48387096774241 epsilon:0.997002999\n",
      "Track generation: 1136..1424 -> 288-tiles track\n",
      "episode:4 frames:1000 total rewards:-33.79790940766605 epsilon:0.993020965034979\n",
      "Track generation: 1177..1475 -> 298-tiles track\n",
      "episode:5 frames:1000 total rewards:-39.393939393939995 epsilon:0.9851045463620021\n",
      "Track generation: 1472..1843 -> 371-tiles track\n",
      "episode:6 frames:1000 total rewards:-45.94594594594669 epsilon:0.9694605362958227\n",
      "Track generation: 1217..1535 -> 318-tiles track\n",
      "episode:7 frames:1000 total rewards:-43.21766561514266 epsilon:0.9389138777035492\n",
      "Track generation: 1127..1413 -> 286-tiles track\n",
      "episode:8 frames:1000 total rewards:-40.350877192983106 epsilon:0.880677710474571\n",
      "Track generation: 1180..1479 -> 299-tiles track\n",
      "episode:9 frames:1000 total rewards:-53.0201342281887 epsilon:0.7748176364970055\n",
      "Track generation: 1107..1396 -> 289-tiles track\n",
      "episode:10 frames:1000 total rewards:-23.611111111111303 epsilon:0.599742027456979\n"
     ]
    }
   ],
   "source": [
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    state = state/255\n",
    "    for frame in range(time):\n",
    "        if epsilon > np.random.rand():\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = bot_racer.predict(np.reshape(state, [1, state.shape[0], state.shape[1], state.shape[2]]))\n",
    "        \n",
    "        action = action.reshape((3))\n",
    "        next_state, reward, done, prob = env.step(action)\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        state = state/255\n",
    "        total_reward = total_reward + reward\n",
    "        \n",
    "        if done:\n",
    "            print('episode:{} frames:{} total rewards:{} epsilon:{}'.format(episode + 1, frame + 1, total_reward, epsilon))\n",
    "            break\n",
    "        \n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon * epsilon_decay\n",
    "        \n",
    "    minibatch = [memory[ii] for ii in np.random.choice(range(len(memory)), batch_size)]\n",
    "    for start, action, reward, next_state, done in minibatch:\n",
    "        Q_target = reward + gamma * bot_racer.predict(np.reshape(next_state, [1, state.shape[0], state.shape[1], state.shape[2]]))\n",
    "        bot_racer.fit(np.reshape(state, [1, state.shape[0], state.shape[1], state.shape[2]]), Q_target, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1060..1329 -> 269-tiles track\n",
      "(96, 96, 3)\n",
      "[[ 0.39381853  0.61909968  0.60351008]]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(state.shape)\n",
    "action = bot_racer.predict(np.reshape(state, [1, state.shape[0], state.shape[1], state.shape[2]]))\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
