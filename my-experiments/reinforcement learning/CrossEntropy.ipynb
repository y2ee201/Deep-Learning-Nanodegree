{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-27 13:01:11,227] Making new env: LunarLander-v2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape\n",
    "print(n_actions)\n",
    "print(n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "for i in range(10000):\n",
    "    new_s, reward, done, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n",
    "print(i)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdul\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "agent = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                      activation='tanh',\n",
    "                      warm_start=True, #keep progress between .fit(...) calls\n",
    "                      max_iter=1 #make only 1 iteration on each .fit(...)\n",
    "                     )\n",
    "#initialize agent to the dimension of state an amount of actions\n",
    "agent.fit([env.reset()]*n_actions,range(n_actions));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_max = 10000\n",
    "def generate_sample():\n",
    "    s = env.reset()\n",
    "    batch_s = []\n",
    "    batch_a = []\n",
    "    total_reward = 0\n",
    "    \n",
    "    for i in range(t_max):\n",
    "        # probs = agent.predict(s.reshape(1, 8))\n",
    "        probs = agent.predict_proba(s.reshape(1, 8))\n",
    "        a = int(np.random.choice(n_actions, 1, p = probs[0]))\n",
    "        new_s, r, done, _ = env.step(a)\n",
    "        batch_s.append(s)\n",
    "        batch_a.append(a)\n",
    "        s = new_s\n",
    "        total_reward = total_reward + r\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    return batch_s, batch_a, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, Mean Reward: -269.72, Threshold: -203.86\n",
      "Iteration: 2, Mean Reward: -214.88, Threshold: -174.57\n",
      "Iteration: 3, Mean Reward: -193.35, Threshold: -163.07\n",
      "Iteration: 4, Mean Reward: -181.44, Threshold: -150.71\n",
      "Iteration: 5, Mean Reward: -179.10, Threshold: -150.28\n",
      "Iteration: 6, Mean Reward: -164.94, Threshold: -146.30\n",
      "Iteration: 7, Mean Reward: -166.23, Threshold: -143.38\n",
      "Iteration: 8, Mean Reward: -164.63, Threshold: -142.32\n",
      "Iteration: 9, Mean Reward: -158.53, Threshold: -137.96\n",
      "Iteration: 10, Mean Reward: -151.76, Threshold: -135.14\n",
      "Iteration: 11, Mean Reward: -146.64, Threshold: -131.94\n",
      "Iteration: 12, Mean Reward: -143.42, Threshold: -128.11\n",
      "Iteration: 13, Mean Reward: -141.43, Threshold: -125.82\n",
      "Iteration: 14, Mean Reward: -137.67, Threshold: -121.26\n",
      "Iteration: 15, Mean Reward: -134.23, Threshold: -119.86\n",
      "Iteration: 16, Mean Reward: -136.16, Threshold: -120.03\n",
      "Iteration: 17, Mean Reward: -136.39, Threshold: -120.20\n",
      "Iteration: 18, Mean Reward: -133.29, Threshold: -117.34\n",
      "Iteration: 19, Mean Reward: -129.45, Threshold: -111.65\n",
      "Iteration: 20, Mean Reward: -116.96, Threshold: -102.58\n",
      "Iteration: 21, Mean Reward: -111.81, Threshold: -97.48\n",
      "Iteration: 22, Mean Reward: -106.67, Threshold: -91.14\n",
      "Iteration: 23, Mean Reward: -103.95, Threshold: -87.00\n",
      "Iteration: 24, Mean Reward: -99.29, Threshold: -85.47\n",
      "Iteration: 25, Mean Reward: -97.34, Threshold: -80.83\n",
      "Iteration: 26, Mean Reward: -92.53, Threshold: -72.83\n",
      "Iteration: 27, Mean Reward: -86.11, Threshold: -69.51\n",
      "Iteration: 28, Mean Reward: -80.60, Threshold: -63.33\n",
      "Iteration: 29, Mean Reward: -75.42, Threshold: -58.15\n",
      "Iteration: 30, Mean Reward: -70.59, Threshold: -50.97\n",
      "Iteration: 31, Mean Reward: -68.76, Threshold: -49.85\n",
      "Iteration: 32, Mean Reward: -61.13, Threshold: -42.77\n",
      "Iteration: 33, Mean Reward: -51.18, Threshold: -33.70\n",
      "Iteration: 34, Mean Reward: -43.07, Threshold: -28.81\n",
      "Iteration: 35, Mean Reward: -39.56, Threshold: -23.69\n",
      "Iteration: 36, Mean Reward: -36.65, Threshold: -23.10\n",
      "Iteration: 37, Mean Reward: -33.14, Threshold: -21.38\n",
      "Iteration: 38, Mean Reward: -29.72, Threshold: -17.45\n",
      "Iteration: 39, Mean Reward: -33.77, Threshold: -18.18\n",
      "Iteration: 40, Mean Reward: -32.35, Threshold: -17.26\n",
      "Iteration: 41, Mean Reward: -31.75, Threshold: -13.29\n",
      "Iteration: 42, Mean Reward: -24.68, Threshold: -10.19\n",
      "Iteration: 43, Mean Reward: -25.64, Threshold: -8.92\n",
      "Iteration: 44, Mean Reward: -23.88, Threshold: -8.31\n",
      "Iteration: 45, Mean Reward: -23.91, Threshold: -8.06\n",
      "Iteration: 46, Mean Reward: -20.34, Threshold: -1.55\n",
      "Iteration: 47, Mean Reward: -17.85, Threshold: -5.08\n",
      "Iteration: 48, Mean Reward: -14.60, Threshold: -2.69\n",
      "Iteration: 49, Mean Reward: -15.16, Threshold: -1.01\n",
      "Iteration: 50, Mean Reward: -13.45, Threshold: -0.35\n",
      "Iteration: 51, Mean Reward: -11.53, Threshold: 1.86\n",
      "Iteration: 52, Mean Reward: -7.97, Threshold: 12.26\n",
      "Iteration: 53, Mean Reward: -6.37, Threshold: 6.34\n",
      "Iteration: 54, Mean Reward: -7.71, Threshold: 8.11\n",
      "Iteration: 55, Mean Reward: -6.05, Threshold: 14.80\n",
      "Iteration: 56, Mean Reward: -10.54, Threshold: 18.15\n",
      "Iteration: 57, Mean Reward: -12.18, Threshold: 11.31\n",
      "Iteration: 58, Mean Reward: -9.79, Threshold: 14.14\n",
      "Iteration: 59, Mean Reward: -5.03, Threshold: 16.74\n",
      "Iteration: 60, Mean Reward: -1.74, Threshold: 27.77\n",
      "Iteration: 61, Mean Reward: -0.54, Threshold: 29.65\n",
      "Iteration: 62, Mean Reward: 2.07, Threshold: 34.06\n",
      "Iteration: 63, Mean Reward: 3.84, Threshold: 36.15\n",
      "Iteration: 64, Mean Reward: 14.03, Threshold: 44.41\n",
      "Iteration: 65, Mean Reward: 15.56, Threshold: 50.69\n",
      "Iteration: 66, Mean Reward: 16.35, Threshold: 50.72\n",
      "Iteration: 67, Mean Reward: 26.99, Threshold: 56.30\n",
      "Iteration: 68, Mean Reward: 17.75, Threshold: 47.04\n",
      "Iteration: 69, Mean Reward: 8.56, Threshold: 46.65\n",
      "Iteration: 70, Mean Reward: 13.78, Threshold: 49.92\n",
      "Iteration: 71, Mean Reward: 17.87, Threshold: 50.34\n",
      "Iteration: 72, Mean Reward: 26.04, Threshold: 56.95\n",
      "Iteration: 73, Mean Reward: 20.07, Threshold: 58.79\n",
      "Iteration: 74, Mean Reward: 24.12, Threshold: 60.84\n",
      "Iteration: 75, Mean Reward: 32.86, Threshold: 62.61\n",
      "Iteration: 76, Mean Reward: 20.36, Threshold: 59.52\n",
      "Iteration: 77, Mean Reward: 25.65, Threshold: 58.96\n",
      "Iteration: 78, Mean Reward: 19.73, Threshold: 53.70\n",
      "Iteration: 79, Mean Reward: 30.65, Threshold: 65.19\n",
      "Iteration: 80, Mean Reward: 30.73, Threshold: 64.52\n",
      "Iteration: 81, Mean Reward: 26.07, Threshold: 62.50\n",
      "Iteration: 82, Mean Reward: 27.76, Threshold: 71.62\n",
      "Iteration: 83, Mean Reward: 29.18, Threshold: 69.42\n",
      "Iteration: 84, Mean Reward: 34.19, Threshold: 67.37\n",
      "Iteration: 85, Mean Reward: 31.08, Threshold: 67.35\n",
      "Iteration: 86, Mean Reward: 32.46, Threshold: 65.26\n",
      "Iteration: 87, Mean Reward: 25.25, Threshold: 67.23\n",
      "Iteration: 88, Mean Reward: 32.50, Threshold: 65.15\n",
      "Iteration: 89, Mean Reward: 33.05, Threshold: 68.15\n",
      "Iteration: 90, Mean Reward: 33.94, Threshold: 62.61\n",
      "Iteration: 91, Mean Reward: 34.96, Threshold: 62.65\n",
      "Iteration: 92, Mean Reward: 35.79, Threshold: 67.68\n",
      "Iteration: 93, Mean Reward: 35.53, Threshold: 62.79\n",
      "Iteration: 94, Mean Reward: 34.33, Threshold: 61.85\n",
      "Iteration: 95, Mean Reward: 26.08, Threshold: 54.09\n",
      "Iteration: 96, Mean Reward: 28.57, Threshold: 55.93\n",
      "Iteration: 97, Mean Reward: 29.68, Threshold: 56.43\n",
      "Iteration: 98, Mean Reward: 28.07, Threshold: 54.67\n",
      "Iteration: 99, Mean Reward: 40.48, Threshold: 68.97\n",
      "Iteration: 100, Mean Reward: 37.26, Threshold: 63.06\n"
     ]
    }
   ],
   "source": [
    "iterations = 100\n",
    "percentile = 70\n",
    "samples = 250\n",
    "\n",
    "for i in range(iterations):\n",
    "    population = [generate_sample() for i in range(samples)]\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*population))\n",
    "    threshold = np.percentile(batch_rewards, percentile)\n",
    "    elite_states = batch_states[batch_rewards > threshold]\n",
    "    elite_actions = batch_actions[batch_rewards > threshold]\n",
    "    elite_states, elite_actions = map(np.concatenate, [elite_states, elite_actions])\n",
    "    agent.fit(X=elite_states, y=elite_actions)\n",
    "    print('Iteration: {0}, Mean Reward: {1:.2f}, Threshold: {2:.2f}'.format(i + 1, np.mean(batch_rewards), threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "s = env.reset()\n",
    "for i in range(1000):\n",
    "    render = env.render('rgb_array')\n",
    "    if i%5==0:\n",
    "        img = Image.fromarray(render, 'RGB')\n",
    "        img.save(''.join(['./renders/',str(i),'.jpg']))\n",
    "    probs = agent.predict_proba(s.reshape(1, 8))\n",
    "    a = int(np.random.choice(n_actions, 1, p = probs[0]))\n",
    "    new_s, reward, done, _ = env.step(a)\n",
    "    s = new_s\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent = Sequential()\n",
    "agent.add(Dense(20, input_shape=n_states, activation='relu'))\n",
    "agent.add(Dense(20, activation='relu'))\n",
    "agent.add(Dense(n_actions, activation='softmax'))\n",
    "agent.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_lookup = np.eye(n_actions)\n",
    "\n",
    "t_max = 10000\n",
    "def generate_sample():\n",
    "    s = env.reset()\n",
    "    batch_s = []\n",
    "    batch_a = []\n",
    "    total_reward = 0\n",
    "    \n",
    "    for i in range(t_max):\n",
    "        probs = agent.predict(s.reshape(1, 8))\n",
    "        a = int(np.random.choice(n_actions, 1, p = probs[0]))\n",
    "        new_s, r, done, _ = env.step(a)\n",
    "        batch_s.append(s)\n",
    "        batch_a.append(action_lookup[a])\n",
    "        s = new_s\n",
    "        total_reward = total_reward + r\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    return batch_s, batch_a, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "percentile = 70\n",
    "samples = 250\n",
    "\n",
    "for i in range(iterations):\n",
    "    population = [generate_sample() for i in range(samples)]\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*population))\n",
    "    threshold = np.percentile(batch_rewards, percentile)\n",
    "    elite_states = batch_states[batch_rewards > threshold]\n",
    "    elite_actions = batch_actions[batch_rewards > threshold]\n",
    "    elite_states, elite_actions = map(np.concatenate, [elite_states, elite_actions])\n",
    "    agent.fit(epochs=1, x=elite_states, y=elite_actions)\n",
    "    print('Iteration: {0}, Mean Reward: {1:.2f}, Threshold: {2:.2f}'.format(i + 1, np.mean(batch_rewards), threshold))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
